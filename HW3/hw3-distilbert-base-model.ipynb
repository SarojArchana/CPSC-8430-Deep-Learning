{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"},"papermill":{"default_parameters":{},"duration":10348.433562,"end_time":"2023-03-31T19:19:56.199949","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2023-03-31T16:27:27.766387","version":"2.4.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"02f548c7e717411ab57c4f14d0aa7f64":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"09947f80d3c54a7c9619d887df84548b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0a53d559b5414604ace112f241962e06":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_cbf62006ceb04824a312764a452e3abb","max":466062,"min":0,"orientation":"horizontal","style":"IPY_MODEL_02f548c7e717411ab57c4f14d0aa7f64","value":466062}},"0ed04fb4c55548bca74e7faa9b081438":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"112ea969a49941aa85b2a90f9471456b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ae9c961d574847168b2b49b8bcbe5e68","placeholder":"​","style":"IPY_MODEL_97371da17eff4c33ad347009fd12adf8","value":"Downloading (…)okenizer_config.json: 100%"}},"17c08b7cd4b54b8e97a942348d05f2c3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"17eb5a39046140a5b62be9b1ae2acf0e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"228f955885ca41d79e39858542d35adb":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"252bffbf05ad43948465465e5a00b011":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"265518e039294a4797697d13495d994c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"312744ca966c4311b263d2c658f87254":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_94caec34bf4746e0abc2ab94236a87f0","max":267967963,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3e31087e9e84409d81d127c5bbd9c6d7","value":267967963}},"32ad03dc098849e3a84d4f1308d12fac":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_265518e039294a4797697d13495d994c","placeholder":"​","style":"IPY_MODEL_73826913cb3643d98dd1060f771d5566","value":" 268M/268M [00:01&lt;00:00, 312MB/s]"}},"33610956e9a94b769d6a480b1d39f856":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7aa40b289cdb426f8dc7f6bede8b3433","IPY_MODEL_0a53d559b5414604ace112f241962e06","IPY_MODEL_4427afc53f0f4f43963f5719c75cbd94"],"layout":"IPY_MODEL_b0e512d317624d41adbd1283c4a54197"}},"33bbaf0f1e2548d2aeff8f463fab2440":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c279fb8480234ac09bd62c3589f53521","placeholder":"​","style":"IPY_MODEL_d27311008b35405daa53f323b9018fe1","value":"Downloading (…)solve/main/vocab.txt: 100%"}},"3e31087e9e84409d81d127c5bbd9c6d7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4427afc53f0f4f43963f5719c75cbd94":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0ed04fb4c55548bca74e7faa9b081438","placeholder":"​","style":"IPY_MODEL_c5acd51be1714a99b6e1092092bb70f4","value":" 466k/466k [00:00&lt;00:00, 1.74MB/s]"}},"4ed4e963ea4a47338917113be2307ebf":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"55275878fa1d4cc0a09e33f8606c5d09":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"55ec235417814031858420294413d493":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e898b6bcf65e4cffb90653a4040611a1","placeholder":"​","style":"IPY_MODEL_76bac2ce478b4f52bf4502ed7773f656","value":"Downloading pytorch_model.bin: 100%"}},"5fcd543273414e4993afe290808bd570":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4ed4e963ea4a47338917113be2307ebf","placeholder":"​","style":"IPY_MODEL_e5cf6576dc644721a34d79a97599177c","value":" 28.0/28.0 [00:00&lt;00:00, 1.16kB/s]"}},"71e6e5d75d284a0888ba1397a75659a0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"73826913cb3643d98dd1060f771d5566":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"76bac2ce478b4f52bf4502ed7773f656":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7aa40b289cdb426f8dc7f6bede8b3433":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_17eb5a39046140a5b62be9b1ae2acf0e","placeholder":"​","style":"IPY_MODEL_09947f80d3c54a7c9619d887df84548b","value":"Downloading (…)/main/tokenizer.json: 100%"}},"83b830f39b9f46f5b5cfa1c1c003a9a2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"84b3e781806b46a6895e8ee4ec8510bd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_71e6e5d75d284a0888ba1397a75659a0","placeholder":"​","style":"IPY_MODEL_c57b8b63c85642488b41fa7029d0c284","value":" 483/483 [00:00&lt;00:00, 29.6kB/s]"}},"8aca188d73f14603867b3e1826731b7a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_17c08b7cd4b54b8e97a942348d05f2c3","max":483,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a554cc10ead24cea83af0fd5bf3eda3c","value":483}},"8f2804ca19df4fc0a594a8693344106f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_55ec235417814031858420294413d493","IPY_MODEL_312744ca966c4311b263d2c658f87254","IPY_MODEL_32ad03dc098849e3a84d4f1308d12fac"],"layout":"IPY_MODEL_228f955885ca41d79e39858542d35adb"}},"8fba97ba463e400db07527ce70c659f2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"920233044bf24386a28c5d45d11abc49":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"94caec34bf4746e0abc2ab94236a87f0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"97371da17eff4c33ad347009fd12adf8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a554cc10ead24cea83af0fd5bf3eda3c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a878752a60714a7ab978136662d12bed":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_afda89287eb943b39d6abffeac78f3a1","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e70d2cefb9694d4dba22e6987572763e","value":231508}},"ae9c961d574847168b2b49b8bcbe5e68":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"af95d79e241840598ebbe4c2f42cfb7f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f1224a2833f54eb69eba395db4e066e8","IPY_MODEL_8aca188d73f14603867b3e1826731b7a","IPY_MODEL_84b3e781806b46a6895e8ee4ec8510bd"],"layout":"IPY_MODEL_bb13e5f3d98a4eaeaf28b1613a6abcd0"}},"afda89287eb943b39d6abffeac78f3a1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b0e512d317624d41adbd1283c4a54197":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b345d5a7b8064abb99229405dcbaae0b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b788b828b9bb4f54b39e4515c1f0e81d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_112ea969a49941aa85b2a90f9471456b","IPY_MODEL_fc696c0af9604f53b6181aa0b4a67692","IPY_MODEL_5fcd543273414e4993afe290808bd570"],"layout":"IPY_MODEL_920233044bf24386a28c5d45d11abc49"}},"bb13e5f3d98a4eaeaf28b1613a6abcd0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c279fb8480234ac09bd62c3589f53521":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c57b8b63c85642488b41fa7029d0c284":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c5acd51be1714a99b6e1092092bb70f4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cbf62006ceb04824a312764a452e3abb":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d27311008b35405daa53f323b9018fe1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d7188f14c2e34ebd9207b3d8755a27c9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"de7e28e178a94fc09e3f3085a6868f25":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_33bbaf0f1e2548d2aeff8f463fab2440","IPY_MODEL_a878752a60714a7ab978136662d12bed","IPY_MODEL_ed47da17a9ef43dc80eb77001b007b7b"],"layout":"IPY_MODEL_f68a348239c445d39faba01c935c4aca"}},"e5cf6576dc644721a34d79a97599177c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e70d2cefb9694d4dba22e6987572763e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e898b6bcf65e4cffb90653a4040611a1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ed47da17a9ef43dc80eb77001b007b7b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d7188f14c2e34ebd9207b3d8755a27c9","placeholder":"​","style":"IPY_MODEL_8fba97ba463e400db07527ce70c659f2","value":" 232k/232k [00:00&lt;00:00, 1.25MB/s]"}},"f1224a2833f54eb69eba395db4e066e8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_55275878fa1d4cc0a09e33f8606c5d09","placeholder":"​","style":"IPY_MODEL_252bffbf05ad43948465465e5a00b011","value":"Downloading (…)lve/main/config.json: 100%"}},"f68a348239c445d39faba01c935c4aca":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fc696c0af9604f53b6181aa0b4a67692":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_83b830f39b9f46f5b5cfa1c1c003a9a2","max":28,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b345d5a7b8064abb99229405dcbaae0b","value":28}}},"version_major":2,"version_minor":0}}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"code","source":"\nimport json\ndef read_squad(path):\n    # open JSON file and load intro dictionary\n    with open(path, 'rb') as f:\n        squad_dict = json.load(f)\n\n    # initialize lists for contexts, questions, and answers\n    contexts = []\n    questions = []\n    answers = []\n    # iterate through all data in squad data\n    for group in squad_dict['data']:\n        for passage in group['paragraphs']:\n            context = passage['context']\n            for qa in passage['qas']:\n                question = qa['question']\n                # check if we need to be extracting from 'answers' or 'plausible_answers'\n                if 'plausible_answers' in qa.keys():\n                    access = 'plausible_answers'\n                else:\n                    access = 'answers'\n                for answer in qa[access]:\n                    # append data to lists\n                    contexts.append(context)\n                    questions.append(question)\n                    answers.append(answer)\n    # return formatted data lists\n    return contexts, questions, answers\n\n# execute our read SQuAD function for training and validation sets\ntrain_contexts, train_questions, train_answers = read_squad('/kaggle/input/spoekqa/spoken_train-v1.1.json')\nval_contexts, val_questions, val_answers = read_squad('/kaggle/input/spoekqa/spoken_test-v1.1.json')","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":0.781253,"end_time":"2023-03-31T16:27:37.509412","exception":false,"start_time":"2023-03-31T16:27:36.728159","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-04-01T14:53:25.417429Z","iopub.execute_input":"2023-04-01T14:53:25.418329Z","iopub.status.idle":"2023-04-01T14:53:26.255333Z","shell.execute_reply.started":"2023-04-01T14:53:25.418282Z","shell.execute_reply":"2023-04-01T14:53:26.254270Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"\ndef add_end_idx(answers, contexts):\n    # loop through each answer-context pair\n    for answer, context in zip(answers, contexts):\n        # gold_text refers to the answer we are expecting to find in context\n        gold_text = answer['text']\n        # we already know the start index\n        start_idx = answer['answer_start']\n        # and ideally this would be the end index...\n        end_idx = start_idx + len(gold_text)\n\n        # ...however, sometimes squad answers are off by a character or two\n        if context[start_idx:end_idx] == gold_text:\n            # if the answer is not off :)\n            answer['answer_end'] = end_idx\n        else:\n            # this means the answer is off by 1-2 tokens\n            for n in [1, 2]:\n                if context[start_idx-n:end_idx-n] == gold_text:\n                    answer['answer_start'] = start_idx - n\n                    answer['answer_end'] = end_idx - n\n            \n# and apply the function to our two answer lists\nadd_end_idx(train_answers, train_contexts)\nadd_end_idx(val_answers, val_contexts)","metadata":{"papermill":{"duration":0.040574,"end_time":"2023-03-31T16:27:37.554266","exception":false,"start_time":"2023-03-31T16:27:37.513692","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-04-01T14:53:34.115871Z","iopub.execute_input":"2023-04-01T14:53:34.116260Z","iopub.status.idle":"2023-04-01T14:53:34.156928Z","shell.execute_reply.started":"2023-04-01T14:53:34.116226Z","shell.execute_reply":"2023-04-01T14:53:34.155916Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"from transformers import DistilBertTokenizerFast\n# initialize the tokenizer\ntokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n# tokenize\ntrain_encodings = tokenizer(train_contexts, train_questions, truncation=True, padding=True)\nval_encodings = tokenizer(val_contexts, val_questions, truncation=True, padding=True)","metadata":{"papermill":{"duration":31.147342,"end_time":"2023-03-31T16:28:08.705672","exception":false,"start_time":"2023-03-31T16:27:37.558330","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-04-01T14:53:36.467793Z","iopub.execute_input":"2023-04-01T14:53:36.468306Z","iopub.status.idle":"2023-04-01T14:54:10.073129Z","shell.execute_reply.started":"2023-04-01T14:53:36.468266Z","shell.execute_reply":"2023-04-01T14:54:10.072028Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d1cccd4f9ee54691beb8cad7aead3ca1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b74bae2bdf5d4680b87561a48ec4e286"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9b7735e8c333470f977c27b62239744f"}},"metadata":{}}]},{"cell_type":"code","source":"\ndef add_token_positions(encodings, answers):\n    # initialize lists to contain the token indices of answer start/end\n    start_positions = []\n    end_positions = []\n    for i in range(len(answers)):\n        # append start/end token position using char_to_token method\n        start_positions.append(encodings.char_to_token(i, answers[i]['answer_start']))\n        end_positions.append(encodings.char_to_token(i, answers[i]['answer_end']))\n\n        # if start position is None, the answer passage has been truncated\n        if start_positions[-1] is None:\n            start_positions[-1] = tokenizer.model_max_length\n        # end position cannot be found, char_to_token found space, so shift position until found\n        shift = 1\n        while end_positions[-1] is None:\n            end_positions[-1] = encodings.char_to_token(i, answers[i]['answer_end'] - shift)\n            shift += 1\n    # update our encodings object with the new token-based start/end positions\n    encodings.update({'start_positions': start_positions, 'end_positions': end_positions})\n\n# apply function to our data\nadd_token_positions(train_encodings, train_answers)\nadd_token_positions(val_encodings, val_answers)","metadata":{"papermill":{"duration":0.229071,"end_time":"2023-03-31T16:28:08.939597","exception":false,"start_time":"2023-03-31T16:28:08.710526","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-04-01T14:54:10.075130Z","iopub.execute_input":"2023-04-01T14:54:10.075700Z","iopub.status.idle":"2023-04-01T14:54:10.324234Z","shell.execute_reply.started":"2023-04-01T14:54:10.075668Z","shell.execute_reply":"2023-04-01T14:54:10.323202Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"\nimport torch\n\nclass SquadDataset(torch.utils.data.Dataset):\n    def __init__(self, encodings):\n        self.encodings = encodings\n\n    def __getitem__(self, idx):\n        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n\n    def __len__(self):\n        return len(self.encodings.input_ids)\n\n# build datasets for both our training and validation sets\ntrain_dataset = SquadDataset(train_encodings)\nval_dataset = SquadDataset(val_encodings)","metadata":{"papermill":{"duration":2.250337,"end_time":"2023-03-31T16:28:11.194319","exception":false,"start_time":"2023-03-31T16:28:08.943982","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-04-01T14:54:10.325644Z","iopub.execute_input":"2023-04-01T14:54:10.326435Z","iopub.status.idle":"2023-04-01T14:54:11.760133Z","shell.execute_reply.started":"2023-04-01T14:54:10.326394Z","shell.execute_reply":"2023-04-01T14:54:11.759114Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"from transformers import DistilBertForQuestionAnswering\nmodel = DistilBertForQuestionAnswering.from_pretrained(\"distilbert-base-uncased\")","metadata":{"papermill":{"duration":4.696702,"end_time":"2023-03-31T16:28:15.896078","exception":false,"start_time":"2023-03-31T16:28:11.199376","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-04-01T14:54:11.762806Z","iopub.execute_input":"2023-04-01T14:54:11.763232Z","iopub.status.idle":"2023-04-01T14:54:13.429911Z","shell.execute_reply.started":"2023-04-01T14:54:11.763189Z","shell.execute_reply":"2023-04-01T14:54:13.428883Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stderr","text":"Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForQuestionAnswering: ['vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight']\n- This IS expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nfrom transformers import AdamW\nfrom tqdm import tqdm\n\n# setup GPU/CPU\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n# move model over to detected device\nmodel.to(device)\n# activate training mode of model\nmodel.train()\n# initialize adam optimizer with weight decay (reduces chance of overfitting)\noptim = AdamW(model.parameters(), lr=2e-6)\n\n# initialize data loader for training data\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n\nfor epoch in range(10):\n    model.train()\n    # setup loop (we use tqdm for the progress bar)\n    loop = tqdm(train_loader, leave=True)\n    for batch in loop:\n        break\n        # initialize calculated gradients (from prev step)\n        optim.zero_grad()\n        # pull all the tensor batches required for training\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        start_positions = batch['start_positions'].to(device)\n        end_positions = batch['end_positions'].to(device)\n        # train model on batch and return outputs (incl. loss)\n        outputs = model(input_ids, attention_mask=attention_mask,\n                        start_positions=start_positions,\n                        end_positions=end_positions)\n        # extract loss\n        loss = outputs[0]\n        # calculate loss for every parameter that needs grad update\n        loss.backward()\n        # update parameters\n        optim.step()\n        # print relevant info to progress bar\n        loop.set_description(f'Epoch {epoch}')\n        loop.set_postfix(loss=loss.item())","metadata":{"papermill":{"duration":9751.79916,"end_time":"2023-03-31T19:10:47.700206","exception":false,"start_time":"2023-03-31T16:28:15.901046","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-04-01T14:54:13.431625Z","iopub.execute_input":"2023-04-01T14:54:13.432525Z","iopub.status.idle":"2023-04-01T14:54:18.541066Z","shell.execute_reply.started":"2023-04-01T14:54:13.432478Z","shell.execute_reply":"2023-04-01T14:54:18.539885Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n  0%|          | 0/2320 [00:00<?, ?it/s]\n  0%|          | 0/2320 [00:00<?, ?it/s]\n  0%|          | 0/2320 [00:00<?, ?it/s]\n  0%|          | 0/2320 [00:00<?, ?it/s]\n  0%|          | 0/2320 [00:00<?, ?it/s]\n  0%|          | 0/2320 [00:00<?, ?it/s]\n  0%|          | 0/2320 [00:00<?, ?it/s]\n  0%|          | 0/2320 [00:00<?, ?it/s]\n  0%|          | 0/2320 [00:00<?, ?it/s]\n  0%|          | 0/2320 [00:00<?, ?it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nif not os.path.exists('../models'):\n   os.makedirs('../models')\nmodel_path = 'models/distilbert-custom'\nmodel.save_pretrained(model_path)\ntokenizer.save_pretrained(model_path)","metadata":{"papermill":{"duration":4.52976,"end_time":"2023-03-31T19:10:55.983287","exception":false,"start_time":"2023-03-31T19:10:51.453527","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = DistilBertForQuestionAnswering.from_pretrained(\"/kaggle/input/hw3-bert-base-model-dataset\")\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2023-04-01T14:54:18.542883Z","iopub.execute_input":"2023-04-01T14:54:18.543278Z","iopub.status.idle":"2023-04-01T14:54:21.408653Z","shell.execute_reply.started":"2023-04-01T14:54:18.543239Z","shell.execute_reply":"2023-04-01T14:54:21.407486Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"DistilBertForQuestionAnswering(\n  (distilbert): DistilBertModel(\n    (embeddings): Embeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (transformer): Transformer(\n      (layer): ModuleList(\n        (0): TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n            (activation): GELUActivation()\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n        (1): TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n            (activation): GELUActivation()\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n        (2): TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n            (activation): GELUActivation()\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n        (3): TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n            (activation): GELUActivation()\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n        (4): TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n            (activation): GELUActivation()\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n        (5): TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n            (activation): GELUActivation()\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n      )\n    )\n  )\n  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n  (dropout): Dropout(p=0.1, inplace=False)\n)"},"metadata":{}}]},{"cell_type":"code","source":"# # switch model out of training mode\n# model.eval()\n\n# #val_sampler = SequentialSampler(val_dataset)\n# val_loader = DataLoader(val_dataset, batch_size=16)\n\n# true_starts = []\n# true_ends = []\n# pred_starts = []\n# pred_ends = []\n\n# # initialize loop for progress bar\n# loop = tqdm(val_loader)\n# # loop through batches\n# for batch in loop:\n#     # we don't need to calculate gradients as we're not training\n#     with torch.no_grad():\n#         # pull batched items from loader\n#         input_ids = batch['input_ids'].to(device)\n#         attention_mask = batch['attention_mask'].to(device)\n#         start_true = batch['start_positions'].to(device)\n#         end_true = batch['end_positions'].to(device)\n#         # make predictions\n#         outputs = model(input_ids, attention_mask=attention_mask)\n#         # pull preds out\n#         start_pred = torch.argmax(outputs['start_logits'], dim=1)\n#         end_pred = torch.argmax(outputs['end_logits'], dim=1)\n#         # append predictions and true values to lists\n#         true_starts.extend(start_true.cpu().numpy())\n#         true_ends.extend(end_true.cpu().numpy())\n#         pred_starts.extend(start_pred.cpu().numpy())\n#         pred_ends.extend(end_pred.cpu().numpy())\n# import numpy as np\n# # calculate precision and recall\n# true_starts = np.array(true_starts)\n# true_ends = np.array(true_ends)\n# pred_starts = np.array(pred_starts)\n# pred_ends = np.array(pred_ends)\n\n# true_pos_starts = np.sum(np.logical_and(true_starts == pred_starts, true_starts != -1))\n# true_pos_ends = np.sum(np.logical_and(true_ends == pred_ends, true_ends != -1))\n# false_pos_starts = np.sum(np.logical_and(true_starts != pred_starts, pred_starts != -1))\n# false_pos_ends = np.sum(np.logical_and(true_ends != pred_ends, pred_ends != -1))\n# false_neg_starts = np.sum(np.logical_and(true_starts != pred_starts, true_starts != -1))\n# false_neg_ends = np.sum(np.logical_and(true_ends != pred_ends, true_ends != -1))\n\n# precision_starts = true_pos_starts / (true_pos_starts + false_pos_starts + 1e-9)\n# recall_starts = true_pos_starts / (true_pos_starts + false_neg_starts + 1e-9)\n# precision_ends = true_pos_ends / (true_pos_ends + false_pos_ends + 1e-9)\n# recall_ends = true_pos_ends / (true_pos_ends + false_neg_ends + 1e-9)\n\n# # calculate F1 score\n# f1_starts = 2 * (precision_starts * recall_starts) / (precision_starts + recall_starts + 1e-9)\n# f1_ends = 2 * (precision_ends * recall_ends) / (precision_ends + recall_ends + 1e-9)\n# f1 = (f1_starts + f1_ends) / 2\n\n# print(\"F1 score: {:.4f}\".format(f1))","metadata":{"papermill":{"duration":140.440368,"end_time":"2023-03-31T19:16:07.922170","exception":false,"start_time":"2023-03-31T19:13:47.481802","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# switch model out of training mode\nmodel.eval()\n\n#val_sampler = SequentialSampler(val_dataset)\nval_loader = DataLoader(val_dataset, batch_size=16)\n\nacc = []\n\n# initialize loop for progress bar\nloop = tqdm(val_loader)\n# loop through batches\nanswers = []\nreferences = []\nfor batch in loop:\n    # we don't need to calculate gradients as we're not training\n    with torch.no_grad():\n        # pull batched items from loader\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        start_true = batch['start_positions'].to(device)\n        end_true = batch['end_positions'].to(device)\n        # make predictions\n        outputs = model(input_ids, attention_mask=attention_mask)\n        # pull preds out\n        start_pred = torch.argmax(outputs['start_logits'], dim=1)\n        end_pred = torch.argmax(outputs['end_logits'], dim=1)\n        # calculate accuracy for both and append to accuracy list\n        acc.append(((start_pred == start_true).sum()/len(start_pred)).item())\n        acc.append(((end_pred == end_true).sum()/len(end_pred)).item())\n        for i in range(start_pred.shape[0]):\n            all_tokens = tokenizer.convert_ids_to_tokens(batch['input_ids'][i])\n            answer = ' '.join(all_tokens[start_pred[i] : end_pred[i]+1])\n            ref = ' '.join(all_tokens[start_true[i] : end_true[i]+1])\n            ans_ids = tokenizer.convert_tokens_to_ids(answer.split())\n            answer = tokenizer.decode(ans_ids)\n            answers.append(answer)\n            references.append(ref)\n# calculate average accuracy in total\n","metadata":{"papermill":{"duration":163.723275,"end_time":"2023-03-31T19:19:03.545505","exception":false,"start_time":"2023-03-31T19:16:19.822230","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-04-01T14:54:42.579615Z","iopub.execute_input":"2023-04-01T14:54:42.580504Z","iopub.status.idle":"2023-04-01T14:57:19.491251Z","shell.execute_reply.started":"2023-04-01T14:54:42.580464Z","shell.execute_reply":"2023-04-01T14:57:19.490005Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"100%|██████████| 993/993 [02:36<00:00,  6.33it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"\nfrom __future__ import print_function\nfrom collections import Counter\nimport string\nimport re\nimport argparse\nimport json\nimport sys\n\n\ndef normalize_answer(s):\n    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n    def remove_articles(text):\n        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n\n    def white_space_fix(text):\n        return ' '.join(text.split())\n\n    def remove_punc(text):\n        exclude = set(string.punctuation)\n        return ''.join(ch for ch in text if ch not in exclude)\n\n    def lower(text):\n        return text.lower()\n\n    return white_space_fix(remove_articles(remove_punc(lower(s))))\n\ndef exact_match_score(prediction, ground_truth):\n    return (normalize_answer(prediction) == normalize_answer(ground_truth))\n\n\ndef metric_max_over_ground_truths(metric_fn, prediction, ground_truths):\n    scores_for_ground_truths = []\n    for ground_truth in ground_truths:\n        score = metric_fn(prediction, ground_truth)\n        scores_for_ground_truths.append(score)\n    if len(scores_for_ground_truths)==0: return 0\n    return max(scores_for_ground_truths)\n\ndef f1_score(prediction, ground_truth):\n    prediction_tokens = normalize_answer(prediction).split()\n    ground_truth_tokens = normalize_answer(ground_truth).split()\n    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n    num_same = sum(common.values())\n    if num_same == 0:\n        return 0\n    precision = 1.0 * num_same / len(prediction_tokens)\n    recall = 1.0 * num_same / len(ground_truth_tokens)\n    f1 = (2 * precision * recall) / (precision + recall)\n    return f1\ndef evaluate(gold_answers, predictions):\n    f1 = exact_match = total = 0\n\n    for ground_truths, prediction in zip(gold_answers, predictions):\n        total += 1\n        exact_match += metric_max_over_ground_truths(\n                    exact_match_score, prediction, ground_truths)\n        f1 += metric_max_over_ground_truths(\n          f1_score, prediction, [ground_truths])\n    \n    exact_match = 100.0 * exact_match / total\n    f1 = 100.0 * f1 / total\n\n    return {'f1': f1}\n     ","metadata":{"papermill":{"duration":4.104956,"end_time":"2023-03-31T19:19:19.802993","exception":false,"start_time":"2023-03-31T19:19:15.698037","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-04-01T14:57:19.493682Z","iopub.execute_input":"2023-04-01T14:57:19.494349Z","iopub.status.idle":"2023-04-01T14:57:19.508251Z","shell.execute_reply.started":"2023-04-01T14:57:19.494310Z","shell.execute_reply":"2023-04-01T14:57:19.507103Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"evaluate(references,answers)","metadata":{"papermill":{"duration":9.13174,"end_time":"2023-03-31T19:19:33.024110","exception":false,"start_time":"2023-03-31T19:19:23.892370","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-04-01T14:57:19.509848Z","iopub.execute_input":"2023-04-01T14:57:19.510284Z","iopub.status.idle":"2023-04-01T14:57:24.495026Z","shell.execute_reply.started":"2023-04-01T14:57:19.510242Z","shell.execute_reply":"2023-04-01T14:57:24.493944Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"{'f1': 52.25009544921429}"},"metadata":{}}]}]}