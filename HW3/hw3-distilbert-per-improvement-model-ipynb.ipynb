{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"},"papermill":{"default_parameters":{},"duration":30058.62845,"end_time":"2023-04-01T05:57:47.121976","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2023-03-31T21:36:48.493526","version":"2.4.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"073b6fc458054894bcf7c0eec25f1fe3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"08f7a7c44486449785dfefa32e6cc292":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1137311308254a7b86f726d0f49cf961":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"13726a63cbc9420c8f413409dc7ef158":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a36014ba62b1435c98c0a79dbab1abd1","placeholder":"​","style":"IPY_MODEL_dfab98ec64e7456383da210d5d60d9ba","value":"Downloading (…)/main/tokenizer.json: 100%"}},"19722d625d0b4dd7b9ee9e652aafa832":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1f1dc276ffa1444f814cf991e2950c59":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"29fa0f13dc7f4a1c8cbfafa73c2de0cb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e6800639bcd548d6a361c6486cb8784c","max":483,"min":0,"orientation":"horizontal","style":"IPY_MODEL_19722d625d0b4dd7b9ee9e652aafa832","value":483}},"3a94f16088bc4fa1bd7c6a9679a2f1e9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3da57623d20e42728d5c99f22c02f31d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4bd224654a3f4dbea6fcc5ead5839f12":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4ef2222bc9824f0d8a094787c095b176":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4f0a6ad5aa5246aaaa057ddfdb17ca29":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d47eee2643194ce091bfa2abc93662cb","IPY_MODEL_6e4deccc98a0471b9ad351cd913d3204","IPY_MODEL_bddfc64c77354a1fa7d0fb1f72accdce"],"layout":"IPY_MODEL_69aa4ff96ca54325a81cb34e735bb896"}},"57c868c4c5174f428f225940e20a567e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"57f6a5e8c73545d99c479609ca095079":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5fa15c2ea94f4ec19c76033561045a5a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"62c32d708c9748d88bdf199790480d30":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_95a3267cd8134dba8181bb92b2fdfce8","IPY_MODEL_d9b4f801a22448fc9e35f70867f62e5e","IPY_MODEL_9740e0882b9a44cbaef26a321c162428"],"layout":"IPY_MODEL_5fa15c2ea94f4ec19c76033561045a5a"}},"68add5e99d2747619e7f500de9bd02a0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"69aa4ff96ca54325a81cb34e735bb896":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6e4deccc98a0471b9ad351cd913d3204":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_abb1e0dcc39a4c2e8079c7f517de62f6","max":28,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d0a7450c8d6048ad9059d13484acdd25","value":28}},"73a8b885939b42f88933f42865033e97":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"791aa98f25ca46faa05e878f04b3e6cf":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"83369bd94ff84941be74a711563f08da":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"95a3267cd8134dba8181bb92b2fdfce8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fff5059adb234615986fba2673be9720","placeholder":"​","style":"IPY_MODEL_f84a850ee9a64ce0bd276e4d7afa6644","value":"Downloading pytorch_model.bin: 100%"}},"9740e0882b9a44cbaef26a321c162428":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_57f6a5e8c73545d99c479609ca095079","placeholder":"​","style":"IPY_MODEL_cbed2681eb7f4378bfe1ccd986fb1cee","value":" 268M/268M [00:01&lt;00:00, 237MB/s]"}},"9a3206cc9ba3497885e1bce2629f7df4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d4d07f2f335442fcbc292a530a35d1d8","IPY_MODEL_29fa0f13dc7f4a1c8cbfafa73c2de0cb","IPY_MODEL_cfb00fae356948d592ebd38a387be16e"],"layout":"IPY_MODEL_73a8b885939b42f88933f42865033e97"}},"a0794e60264f408f9cbad42d48ff4248":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_13726a63cbc9420c8f413409dc7ef158","IPY_MODEL_bf4c7b3ed44f4eba8131e101a3915f28","IPY_MODEL_f9ae08deba8e4475a2ff195bf7dccc30"],"layout":"IPY_MODEL_e45e5de010da4c9fbeb2597213d2089b"}},"a36014ba62b1435c98c0a79dbab1abd1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"abb1e0dcc39a4c2e8079c7f517de62f6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b521eca94d0f4989bdd9b24fbabfb2be":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bddfc64c77354a1fa7d0fb1f72accdce":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1f1dc276ffa1444f814cf991e2950c59","placeholder":"​","style":"IPY_MODEL_c1012e2fa8dd4162a7b9566c8fd03dab","value":" 28.0/28.0 [00:00&lt;00:00, 998B/s]"}},"bf4c7b3ed44f4eba8131e101a3915f28":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e21c7bc7e72b4745aa7a3de351d58c7f","max":466062,"min":0,"orientation":"horizontal","style":"IPY_MODEL_57c868c4c5174f428f225940e20a567e","value":466062}},"c1012e2fa8dd4162a7b9566c8fd03dab":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c2336f2497394501861486b1664293ba":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c6b44bb75a99416a88f1d726d8bba7cb":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cbed2681eb7f4378bfe1ccd986fb1cee":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cf8b2bd7c65f4e96a3f3c881ed585969":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c6b44bb75a99416a88f1d726d8bba7cb","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d72bd9b9dc4a4eeb9cfa739906b00989","value":231508}},"cfb00fae356948d592ebd38a387be16e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4ef2222bc9824f0d8a094787c095b176","placeholder":"​","style":"IPY_MODEL_073b6fc458054894bcf7c0eec25f1fe3","value":" 483/483 [00:00&lt;00:00, 28.9kB/s]"}},"d0a7450c8d6048ad9059d13484acdd25":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d47eee2643194ce091bfa2abc93662cb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_791aa98f25ca46faa05e878f04b3e6cf","placeholder":"​","style":"IPY_MODEL_08f7a7c44486449785dfefa32e6cc292","value":"Downloading (…)okenizer_config.json: 100%"}},"d4d07f2f335442fcbc292a530a35d1d8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c2336f2497394501861486b1664293ba","placeholder":"​","style":"IPY_MODEL_1137311308254a7b86f726d0f49cf961","value":"Downloading (…)lve/main/config.json: 100%"}},"d72bd9b9dc4a4eeb9cfa739906b00989":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d881dc56fdf84389954d5f2077b2266e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ed9cc6687f184798a92ebc661bc66f68","placeholder":"​","style":"IPY_MODEL_3a94f16088bc4fa1bd7c6a9679a2f1e9","value":"Downloading (…)solve/main/vocab.txt: 100%"}},"d9b4f801a22448fc9e35f70867f62e5e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b521eca94d0f4989bdd9b24fbabfb2be","max":267967963,"min":0,"orientation":"horizontal","style":"IPY_MODEL_68add5e99d2747619e7f500de9bd02a0","value":267967963}},"dfab98ec64e7456383da210d5d60d9ba":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e21c7bc7e72b4745aa7a3de351d58c7f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e45e5de010da4c9fbeb2597213d2089b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e6800639bcd548d6a361c6486cb8784c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e7faca5cef0f4d8e9221f406d4503ede":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ed9cc6687f184798a92ebc661bc66f68":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f3b269c431bf48cd88b8894784bece2a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3da57623d20e42728d5c99f22c02f31d","placeholder":"​","style":"IPY_MODEL_e7faca5cef0f4d8e9221f406d4503ede","value":" 232k/232k [00:00&lt;00:00, 1.24MB/s]"}},"f84a850ee9a64ce0bd276e4d7afa6644":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f9ae08deba8e4475a2ff195bf7dccc30":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fb0e4b8e876d4c309857dafd089cbd1a","placeholder":"​","style":"IPY_MODEL_4bd224654a3f4dbea6fcc5ead5839f12","value":" 466k/466k [00:00&lt;00:00, 1.75MB/s]"}},"fb0e4b8e876d4c309857dafd089cbd1a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fe0ac8ce5ebf4ea1a3001335bf126e4c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d881dc56fdf84389954d5f2077b2266e","IPY_MODEL_cf8b2bd7c65f4e96a3f3c881ed585969","IPY_MODEL_f3b269c431bf48cd88b8894784bece2a"],"layout":"IPY_MODEL_83369bd94ff84941be74a711563f08da"}},"fff5059adb234615986fba2673be9720":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}},"version_major":2,"version_minor":0}}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"code","source":"\nimport json\ndef read_squad(path):\n    # open JSON file and load intro dictionary\n    with open(path, 'rb') as f:\n        squad_dict = json.load(f)\n\n    # initialize lists for contexts, questions, and answers\n    contexts = []\n    questions = []\n    answers = []\n    # iterate through all data in squad data\n    for group in squad_dict['data']:\n        for passage in group['paragraphs']:\n            context = passage['context']\n            for qa in passage['qas']:\n                question = qa['question']\n                # check if we need to be extracting from 'answers' or 'plausible_answers'\n                if 'plausible_answers' in qa.keys():\n                    access = 'plausible_answers'\n                else:\n                    access = 'answers'\n                for answer in qa[access]:\n                    # append data to lists\n                    contexts.append(context)\n                    questions.append(question)\n                    answers.append(answer)\n    # return formatted data lists\n    return contexts, questions, answers\n\n# execute our read SQuAD function for training and validation sets\ntrain_contexts, train_questions, train_answers = read_squad('/kaggle/input/spoekqa/spoken_train-v1.1.json')\nval_contexts, val_questions, val_answers = read_squad('/kaggle/input/spoekqa/spoken_test-v1.1.json')","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":0.78387,"end_time":"2023-03-31T21:36:58.918162","exception":false,"start_time":"2023-03-31T21:36:58.134292","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-04-01T15:11:14.557375Z","iopub.execute_input":"2023-04-01T15:11:14.558102Z","iopub.status.idle":"2023-04-01T15:11:15.802246Z","shell.execute_reply.started":"2023-04-01T15:11:14.558058Z","shell.execute_reply":"2023-04-01T15:11:15.801093Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"\ndef add_end_idx(answers, contexts):\n    # loop through each answer-context pair\n    for answer, context in zip(answers, contexts):\n        # gold_text refers to the answer we are expecting to find in context\n        gold_text = answer['text']\n        # we already know the start index\n        start_idx = answer['answer_start']\n        # and ideally this would be the end index...\n        end_idx = start_idx + len(gold_text)\n\n        # ...however, sometimes squad answers are off by a character or two\n        if context[start_idx:end_idx] == gold_text:\n            # if the answer is not off :)\n            answer['answer_end'] = end_idx\n        else:\n            # this means the answer is off by 1-2 tokens\n            for n in [1, 2]:\n                if context[start_idx-n:end_idx-n] == gold_text:\n                    answer['answer_start'] = start_idx - n\n                    answer['answer_end'] = end_idx - n\n            \n# and apply the function to our two answer lists\nadd_end_idx(train_answers, train_contexts)\nadd_end_idx(val_answers, val_contexts)","metadata":{"papermill":{"duration":0.045238,"end_time":"2023-03-31T21:36:58.968170","exception":false,"start_time":"2023-03-31T21:36:58.922932","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-04-01T15:11:15.808676Z","iopub.execute_input":"2023-04-01T15:11:15.811470Z","iopub.status.idle":"2023-04-01T15:11:15.869367Z","shell.execute_reply.started":"2023-04-01T15:11:15.811426Z","shell.execute_reply":"2023-04-01T15:11:15.868260Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"from transformers import DistilBertTokenizerFast\n# initialize the tokenizer\ntokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n# tokenize\ntrain_encodings = tokenizer(train_contexts, train_questions, truncation=True, padding=True)\nval_encodings = tokenizer(val_contexts, val_questions, truncation=True, padding=True)","metadata":{"papermill":{"duration":34.463644,"end_time":"2023-03-31T21:37:33.436228","exception":false,"start_time":"2023-03-31T21:36:58.972584","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-04-01T15:11:15.874179Z","iopub.execute_input":"2023-04-01T15:11:15.876694Z","iopub.status.idle":"2023-04-01T15:12:00.842841Z","shell.execute_reply.started":"2023-04-01T15:11:15.876654Z","shell.execute_reply":"2023-04-01T15:12:00.841649Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"797e6292419246bcb7285017651f5660"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec46e26c0d0f400483d099ed803c8114"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0b66b38a4d2e4701a1e3f2f5da39a880"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d83c7ce9510748c1b317f2557a43d792"}},"metadata":{}}]},{"cell_type":"code","source":"\ndef add_token_positions(encodings, answers):\n    # initialize lists to contain the token indices of answer start/end\n    start_positions = []\n    end_positions = []\n    for i in range(len(answers)):\n        # append start/end token position using char_to_token method\n        start_positions.append(encodings.char_to_token(i, answers[i]['answer_start']))\n        end_positions.append(encodings.char_to_token(i, answers[i]['answer_end']))\n\n        # if start position is None, the answer passage has been truncated\n        if start_positions[-1] is None:\n            start_positions[-1] = tokenizer.model_max_length\n        # end position cannot be found, char_to_token found space, so shift position until found\n        shift = 1\n        while end_positions[-1] is None:\n            end_positions[-1] = encodings.char_to_token(i, answers[i]['answer_end'] - shift)\n            shift += 1\n    # update our encodings object with the new token-based start/end positions\n    encodings.update({'start_positions': start_positions, 'end_positions': end_positions})\n\n# apply function to our data\nadd_token_positions(train_encodings, train_answers)\nadd_token_positions(val_encodings, val_answers)","metadata":{"papermill":{"duration":0.253565,"end_time":"2023-03-31T21:37:33.695074","exception":false,"start_time":"2023-03-31T21:37:33.441509","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-04-01T15:12:00.845702Z","iopub.execute_input":"2023-04-01T15:12:00.846431Z","iopub.status.idle":"2023-04-01T15:12:01.087240Z","shell.execute_reply.started":"2023-04-01T15:12:00.846388Z","shell.execute_reply":"2023-04-01T15:12:01.086156Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"\nimport torch\n\nclass SquadDataset(torch.utils.data.Dataset):\n    def __init__(self, encodings):\n        self.encodings = encodings\n\n    def __getitem__(self, idx):\n        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n\n    def __len__(self):\n        return len(self.encodings.input_ids)\n\n# build datasets for both our training and validation sets\ntrain_dataset = SquadDataset(train_encodings)\nval_dataset = SquadDataset(val_encodings)","metadata":{"papermill":{"duration":2.183805,"end_time":"2023-03-31T21:37:35.884058","exception":false,"start_time":"2023-03-31T21:37:33.700253","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-04-01T15:12:01.088812Z","iopub.execute_input":"2023-04-01T15:12:01.089174Z","iopub.status.idle":"2023-04-01T15:12:03.607511Z","shell.execute_reply.started":"2023-04-01T15:12:01.089136Z","shell.execute_reply":"2023-04-01T15:12:03.606452Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"from transformers import DistilBertForQuestionAnswering\nmodel = DistilBertForQuestionAnswering.from_pretrained(\"distilbert-base-uncased\")","metadata":{"papermill":{"duration":5.070965,"end_time":"2023-03-31T21:37:40.960124","exception":false,"start_time":"2023-03-31T21:37:35.889159","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-04-01T15:12:03.609142Z","iopub.execute_input":"2023-04-01T15:12:03.609540Z","iopub.status.idle":"2023-04-01T15:12:07.455060Z","shell.execute_reply.started":"2023-04-01T15:12:03.609489Z","shell.execute_reply":"2023-04-01T15:12:07.453945Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"131620f4f0fa4d9e9ebdfd0a29b28ca4"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForQuestionAnswering: ['vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_projector.bias']\n- This IS expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nfrom transformers import AdamW, get_linear_schedule_with_warmup\nfrom tqdm import tqdm\nfrom accelerate import Accelerator\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nmodel.to(device)\nmodel.train()\noptim = AdamW(model.parameters(), lr=2e-6)\n# initialize data loader for training data\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n\n# initialize scheduler\nnum_training_steps = len(train_loader) * 30\nscheduler = get_linear_schedule_with_warmup(\n    optim, num_warmup_steps=0, num_training_steps=num_training_steps\n)\naccelerator = Accelerator()\nmodel, optimizer, training_dataloader, scheduler = accelerator.prepare(model, optim, train_loader, scheduler)\nfor epoch in range(30):\n    model.train()\n    # setup loop (we use tqdm for the progress bar)\n    loop = tqdm(train_loader, leave=True)\n    for batch in loop:\n        # initialize calculated gradients (from prev step)\n        optim.zero_grad()\n        # pull all the tensor batches required for training\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        start_positions = batch['start_positions'].to(device)\n        end_positions = batch['end_positions'].to(device)\n        # train model on batch and return outputs (incl. loss)\n        outputs = model(input_ids, attention_mask=attention_mask,\n                        start_positions=start_positions,\n                        end_positions=end_positions)\n        # extract loss\n        loss = outputs[0]\n        accelerator.backward(loss)\n        optim.step()\n        scheduler.step()\n        # print relevant info to progress bar\n        loop.set_description(f'Epoch {epoch}')\n        loop.set_postfix(loss=loss.item(), lr=optim.param_groups[0]['lr'])\n","metadata":{"papermill":{"duration":29271.666063,"end_time":"2023-04-01T05:45:32.646452","exception":false,"start_time":"2023-03-31T21:37:40.980389","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-04-01T15:12:24.181348Z","iopub.execute_input":"2023-04-01T15:12:24.182467Z","iopub.status.idle":"2023-04-01T15:12:28.741851Z","shell.execute_reply.started":"2023-04-01T15:12:24.182399Z","shell.execute_reply":"2023-04-01T15:12:28.739687Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nif not os.path.exists('../models'):\n   os.makedirs('../models')\nmodel_path = 'models/distilbert-custom'\nmodel.save_pretrained(model_path)\ntokenizer.save_pretrained(model_path)","metadata":{"papermill":{"duration":13.020849,"end_time":"2023-04-01T05:45:58.797422","exception":false,"start_time":"2023-04-01T05:45:45.776573","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = DistilBertForQuestionAnswering.from_pretrained(\"/kaggle/input/hw3-distilbert-per-improvement-dataset\")\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2023-04-01T15:13:39.215673Z","iopub.execute_input":"2023-04-01T15:13:39.216124Z","iopub.status.idle":"2023-04-01T15:13:42.200241Z","shell.execute_reply.started":"2023-04-01T15:13:39.216084Z","shell.execute_reply":"2023-04-01T15:13:42.199176Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"DistilBertForQuestionAnswering(\n  (distilbert): DistilBertModel(\n    (embeddings): Embeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (transformer): Transformer(\n      (layer): ModuleList(\n        (0): TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n            (activation): GELUActivation()\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n        (1): TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n            (activation): GELUActivation()\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n        (2): TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n            (activation): GELUActivation()\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n        (3): TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n            (activation): GELUActivation()\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n        (4): TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n            (activation): GELUActivation()\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n        (5): TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n            (activation): GELUActivation()\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n      )\n    )\n  )\n  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n  (dropout): Dropout(p=0.1, inplace=False)\n)"},"metadata":{}}]},{"cell_type":"code","source":"# # switch model out of training mode\n# model.eval()\n\n# #val_sampler = SequentialSampler(val_dataset)\n# val_loader = DataLoader(val_dataset, batch_size=16)\n\n# true_starts = []\n# true_ends = []\n# pred_starts = []\n# pred_ends = []\n\n# # initialize loop for progress bar\n# loop = tqdm(val_loader)\n# # loop through batches\n# for batch in loop:\n#     # we don't need to calculate gradients as we're not training\n#     with torch.no_grad():\n#         # pull batched items from loader\n#         input_ids = batch['input_ids'].to(device)\n#         attention_mask = batch['attention_mask'].to(device)\n#         start_true = batch['start_positions'].to(device)\n#         end_true = batch['end_positions'].to(device)\n#         # make predictions\n#         outputs = model(input_ids, attention_mask=attention_mask)\n#         # pull preds out\n#         start_pred = torch.argmax(outputs['start_logits'], dim=1)\n#         end_pred = torch.argmax(outputs['end_logits'], dim=1)\n#         # append predictions and true values to lists\n#         true_starts.extend(start_true.cpu().numpy())\n#         true_ends.extend(end_true.cpu().numpy())\n#         pred_starts.extend(start_pred.cpu().numpy())\n#         pred_ends.extend(end_pred.cpu().numpy())\n# import numpy as np\n# true_starts = np.array(true_starts)\n# true_ends = np.array(true_ends)\n# pred_starts = np.array(pred_starts)\n# pred_ends = np.array(pred_ends)\n\n# true_pos_starts = np.sum(np.logical_and(true_starts == pred_starts, true_starts != -1))\n# true_pos_ends = np.sum(np.logical_and(true_ends == pred_ends, true_ends != -1))\n# false_pos_starts = np.sum(np.logical_and(true_starts != pred_starts, pred_starts != -1))\n# false_pos_ends = np.sum(np.logical_and(true_ends != pred_ends, pred_ends != -1))\n# false_neg_starts = np.sum(np.logical_and(true_starts != pred_starts, true_starts != -1))\n# false_neg_ends = np.sum(np.logical_and(true_ends != pred_ends, true_ends != -1))\n\n# precision_starts = true_pos_starts / (true_pos_starts + false_pos_starts + 1e-9)\n# recall_starts = true_pos_starts / (true_pos_starts + false_neg_starts + 1e-9)\n# precision_ends = true_pos_ends / (true_pos_ends + false_pos_ends + 1e-9)\n# recall_ends = true_pos_ends / (true_pos_ends + false_neg_ends + 1e-9)\n\n# # calculate F1 score\n# f1_starts = 2 * (precision_starts * recall_starts) / (precision_starts + recall_starts + 1e-9)\n# f1_ends = 2 * (precision_ends * recall_ends) / (precision_ends + recall_ends + 1e-9)\n# f1 = (f1_starts + f1_ends) / 2\n\n# print(\"F1 score: {:.4f}\".format(f1))","metadata":{"papermill":{"duration":150.881162,"end_time":"2023-04-01T05:51:47.559014","exception":false,"start_time":"2023-04-01T05:49:16.677852","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# switch model out of training mode\nmodel.eval()\n\n#val_sampler = SequentialSampler(val_dataset)\nval_loader = DataLoader(val_dataset, batch_size=16)\n\nacc = []\n\n# initialize loop for progress bar\nloop = tqdm(val_loader)\n# loop through batches\nanswers = []\nreferences = []\nfor batch in loop:\n    # we don't need to calculate gradients as we're not training\n    with torch.no_grad():\n        # pull batched items from loader\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        start_true = batch['start_positions'].to(device)\n        end_true = batch['end_positions'].to(device)\n        # make predictions\n        outputs = model(input_ids, attention_mask=attention_mask)\n        # pull preds out\n        start_pred = torch.argmax(outputs['start_logits'], dim=1)\n        end_pred = torch.argmax(outputs['end_logits'], dim=1)\n        # calculate accuracy for both and append to accuracy list\n        acc.append(((start_pred == start_true).sum()/len(start_pred)).item())\n        acc.append(((end_pred == end_true).sum()/len(end_pred)).item())\n        for i in range(start_pred.shape[0]):\n            all_tokens = tokenizer.convert_ids_to_tokens(batch['input_ids'][i])\n            answer = ' '.join(all_tokens[start_pred[i] : end_pred[i]+1])\n            ref = ' '.join(all_tokens[start_true[i] : end_true[i]+1])\n            ans_ids = tokenizer.convert_tokens_to_ids(answer.split())\n            answer = tokenizer.decode(ans_ids)\n            answers.append(answer)\n            references.append(ref)","metadata":{"papermill":{"duration":172.646444,"end_time":"2023-04-01T05:55:18.237235","exception":false,"start_time":"2023-04-01T05:52:25.590791","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-04-01T15:13:43.748300Z","iopub.execute_input":"2023-04-01T15:13:43.748665Z","iopub.status.idle":"2023-04-01T15:16:20.976474Z","shell.execute_reply.started":"2023-04-01T15:13:43.748634Z","shell.execute_reply":"2023-04-01T15:16:20.974842Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"100%|██████████| 993/993 [02:37<00:00,  6.32it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"\nfrom __future__ import print_function\nfrom collections import Counter\nimport string\nimport re\nimport argparse\nimport json\nimport sys\n\n\ndef normalize_answer(s):\n    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n    def remove_articles(text):\n        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n\n    def white_space_fix(text):\n        return ' '.join(text.split())\n\n    def remove_punc(text):\n        exclude = set(string.punctuation)\n        return ''.join(ch for ch in text if ch not in exclude)\n\n    def lower(text):\n        return text.lower()\n\n    return white_space_fix(remove_articles(remove_punc(lower(s))))\n\ndef exact_match_score(prediction, ground_truth):\n    return (normalize_answer(prediction) == normalize_answer(ground_truth))\n\n\ndef metric_max_over_ground_truths(metric_fn, prediction, ground_truths):\n    scores_for_ground_truths = []\n    for ground_truth in ground_truths:\n        score = metric_fn(prediction, ground_truth)\n        scores_for_ground_truths.append(score)\n    if len(scores_for_ground_truths)==0: return 0\n    return max(scores_for_ground_truths)\n\ndef f1_score(prediction, ground_truth):\n    prediction_tokens = normalize_answer(prediction).split()\n    ground_truth_tokens = normalize_answer(ground_truth).split()\n    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n    num_same = sum(common.values())\n    if num_same == 0:\n        return 0\n    precision = 1.0 * num_same / len(prediction_tokens)\n    recall = 1.0 * num_same / len(ground_truth_tokens)\n    f1 = (2 * precision * recall) / (precision + recall)\n    return f1\ndef evaluate(gold_answers, predictions):\n    f1 = exact_match = total = 0\n\n    for ground_truths, prediction in zip(gold_answers, predictions):\n        total += 1\n        exact_match += metric_max_over_ground_truths(\n                    exact_match_score, prediction, ground_truths)\n        f1 += metric_max_over_ground_truths(\n          f1_score, prediction, [ground_truths])\n    \n    exact_match = 100.0 * exact_match / total\n    f1 = 100.0 * f1 / total\n\n    return {'f1': f1}\n     ","metadata":{"papermill":{"duration":12.558907,"end_time":"2023-04-01T05:56:09.250378","exception":false,"start_time":"2023-04-01T05:55:56.691471","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-04-01T15:16:38.327189Z","iopub.execute_input":"2023-04-01T15:16:38.327906Z","iopub.status.idle":"2023-04-01T15:16:38.341483Z","shell.execute_reply.started":"2023-04-01T15:16:38.327869Z","shell.execute_reply":"2023-04-01T15:16:38.340264Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"evaluate(references,answers)","metadata":{"papermill":{"duration":17.233723,"end_time":"2023-04-01T05:56:39.483856","exception":false,"start_time":"2023-04-01T05:56:22.250133","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-04-01T15:16:38.586273Z","iopub.execute_input":"2023-04-01T15:16:38.587180Z","iopub.status.idle":"2023-04-01T15:16:43.066673Z","shell.execute_reply.started":"2023-04-01T15:16:38.587139Z","shell.execute_reply":"2023-04-01T15:16:43.065518Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"{'f1': 54.054424303258635}"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":12.59984,"end_time":"2023-04-01T05:57:31.023367","exception":false,"start_time":"2023-04-01T05:57:18.423527","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]}]}